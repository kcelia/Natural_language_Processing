{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Challenge - Sarcasm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXfAtekn_0Sh",
        "outputId": "59460531-36d2-41e3-e6ed-4ee5f2043293"
      },
      "source": [
        "# Libraries\n",
        "\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "import string\n",
        "import warnings\n",
        "import itertools\n",
        "\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud, ImageColorGenerator\n",
        "from __future__ import print_function # Print each element on a separate line \n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import files\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import torch\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, Dense, Flatten, SpatialDropout1D, Dropout, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "\n",
        "!python -m spacy download en_core_web_lg\n",
        "import en_core_web_lg\n",
        "\n",
        "!pip install git+https://github.com/laxmimerit/preprocess_kgptalkie.git\n",
        "import preprocess_kgptalkie as kgp\n",
        "\n",
        "!pip install transformers\n",
        "from transformers import DistilBertTokenizer, DistilBertModel"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 1.3 MB/s \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiRgggquCZz1"
      },
      "source": [
        "# Set up parameters\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.style.use('ggplot') # seanborn\n",
        "plt.rcParams['figure.dpi'] = 80\n",
        "plt.rcParams['figure.figsize'] = [15,5]\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "tqdm.pandas()\n",
        "\n",
        "nlp = en_core_web_lg.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abDHUXEycxtQ"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2_z-gAh_1eu"
      },
      "source": [
        "# Load train data \n",
        "\n",
        "df_train = pd.read_excel('train.xlsm')\n",
        "\n",
        "df_train.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v08XBOoCbmn"
      },
      "source": [
        "# Load test data \n",
        "\n",
        "df_test = pd.read_excel('test.xlsm')\n",
        "\n",
        "df_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM3uNWKnhLLa"
      },
      "source": [
        "# Exploring the train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z42Gaj2QhN4f"
      },
      "source": [
        "df_train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkodY8VpCoev"
      },
      "source": [
        "df_test.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RR7h6c6ehSTx"
      },
      "source": [
        "# Number of examples per class\n",
        "\n",
        "print(f\"Number of example per class :\\n\", df_train.is_sarcastic.value_counts().sort_index())\n",
        "\n",
        "plt.figure()\n",
        "sns.countplot('is_sarcastic', data=df_train)\n",
        "plt.title('Number of examples per class')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zudiWGv0Vs-J"
      },
      "source": [
        "# Visualize the most occuring words\n",
        "\n",
        "def plt_word_cloud(df, labelcol='class', textcol='text'):\n",
        "\n",
        "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 6)) #, constrained_layout=True)\n",
        "    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
        "    fig.suptitle('Most frequent words per class', fontsize=20, y=0.9)\n",
        "\n",
        "    for label, ax in enumerate(axs.flat):\n",
        "        words      = kgp.get_word_freqs(df[df[labelcol] == label], textcol)\n",
        "        words      =  ' '.join(words.index)\n",
        "        word_cloud = WordCloud(max_font_size=100).generate(words)\n",
        "        ax.imshow(word_cloud, cmap='viridis')\n",
        "        ax.set_title(f\"Class: {label}\\n\")\n",
        "        ax.axis('off')\n",
        "        ax.grid(False)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plt_word_cloud(df=df_train, labelcol='is_sarcastic', textcol='headline')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5fFtTsqcueS"
      },
      "source": [
        "# Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agmv4fdSdZ3j"
      },
      "source": [
        "def preprocessing(df, usecols=[], nb=5, verbose=True, replace_col=False, lemmatiation=False):\n",
        "\n",
        "    text_col = usecols[0] if replace_col else 'preprocessed_text'\n",
        "\n",
        "    ########################################## Preprocessing of the second column\n",
        "    if len(usecols) > 1:\n",
        "        df['links'] = df[usecols[1]].apply(lambda x: ' '.join(re.findall(r'[a-z]+', x.split('/')[-1])))\n",
        "        # Fusinning the column 1 and the column 2 to have as much information us we can \n",
        "        df[text_col] = df['links'] + \" \" + df[usecols[0]] \n",
        "    else:\n",
        "        df[text_col] = df[usecols[0]] \n",
        "\n",
        "    ########################################## Preprocessing \n",
        "    if verbose: \n",
        "        print(f\"{df.columns}\")\n",
        "        print(f\"DF before:\\n{df.head(nb)}\\n\")\n",
        "    \n",
        "    non_charac, digits = set(), set()\n",
        "    for x in df[usecols[0]]:\n",
        "        non_charac |= set(re.findall(r'\\W', x))   \n",
        "        digits     |= set(re.findall(r'\\d+\\s*\\d+', x))       \n",
        "\n",
        "    # Remove the Non-alphanumeric\n",
        "    if verbose: \n",
        "        print(f\"Remove all these non-alphanumeric characters :\\n{non_charac}\")\n",
        "    df[text_col] = df[text_col].apply(lambda x: re.sub(r'\\W', ' ', x))\n",
        "\n",
        "    # Remove digits\n",
        "    if verbose: \n",
        "        print(f\"Remove all digits :\\n{digits}\")\n",
        "    df[text_col] = df[text_col].apply(lambda x: re.sub(r'\\d+\\s*\\d+', 'digit', x))\n",
        "   \n",
        "    # Lower case\n",
        "    df[text_col] = df[text_col].apply(lambda x: x.lower())\n",
        "   \n",
        "    # Remove stopwords\n",
        "    df[text_col] = df[text_col].apply(lambda x: re.sub(r'\\b(' + \\\n",
        "                                                       r'|'.join(nlp.Defaults.stop_words) + \\\n",
        "                                                       r')\\b', ' ', x))\n",
        "    # Remove words with length 1 or 2 in the words\n",
        "    df[text_col] = df[text_col].apply(lambda x: re.sub(r'\\b\\w{1,2}\\b', ' ', x))\n",
        "   \n",
        "    # Remove the months\n",
        "    df[text_col] = df[text_col].apply(lambda x: re.sub(r'january|february|march|april|' + \n",
        "                                                       r'may|june|july|august|september|' +\n",
        "                                                       r'october|november|december',\n",
        "                                                       'month', x))\n",
        "    # Lemmatiation\n",
        "    if lemmatiation:\n",
        "        df[text_col] = df[text_col].apply(lambda x: ' '.join([word.lemma_ for word in nlp(x)]))\n",
        "   \n",
        "    # Remove the extra spaces at the middle, the beginning and the end of the text    \n",
        "    df[text_col] = df[text_col].apply(lambda x: re.sub(r'\\s+', ' ', x))        \n",
        "    df[text_col] = df[text_col].apply(lambda x: re.sub(r'(^\\s+|\\s+$)', ' ', x)) \n",
        "\n",
        "    # Remove duplicates \n",
        "    df[text_col] = df[text_col].apply(lambda x: re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', x))\n",
        "    \n",
        "    if verbose: \n",
        "        print(f\"\\n-------------------\\n{df.columns}\")\n",
        "        print(f\"DF after:\\n{df.head(nb)}\\n\")\n",
        "\n",
        "    # Shuffle the dataframe\n",
        "    df = df.sample(frac=1)\n",
        "    \n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVJmFWwpc1jq"
      },
      "source": [
        "df_train_preproc = preprocessing(df_train.copy(), usecols=['headline'], \n",
        "                                 nb=1, verbose=1, lemmatiation=False)\n",
        "\n",
        "df_train_preproc.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oefDv9SMAG7I"
      },
      "source": [
        "df_test_preproc = preprocessing(df_test.copy(), usecols=['headline'], \n",
        "                                nb=1, verbose=False, lemmatiation=False)\n",
        "df_test_preproc.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxNMu3PfC6iO"
      },
      "source": [
        " # Create the sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8QU0rpNKMBM"
      },
      "source": [
        "# Split the data\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(df_train_preproc['preprocessed_text'], \n",
        "                                                    df_train_preproc['is_sarcastic'], \n",
        "                                                    test_size=0.2, shuffle=True, random_state=42)\n",
        "\n",
        "vocab_size, embedding_dim, max_length= 700, 42, 130\n",
        "trunc_type, padding_type, oov_tok ='post','post', \"<OOV>\"\n",
        "\n",
        "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cCCgt6uvgma"
      },
      "source": [
        "# Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "\"\"\"\n",
        " 'passes': 925,\n",
        " 'thanksgiving': 926,\n",
        " ...\n",
        "\"\"\"\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRt2OcI20Q3R"
      },
      "source": [
        "# Train set preprocessed --> Dict({word: Id}) --> Matrix\n",
        "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "x_train_padded  = pad_sequences(train_sequences, maxlen=max_length, value=0)\n",
        "\n",
        "print(f\"Train set:\\n\" + \\\n",
        "      f\"{type(train_sequences)}, {train_sequences[0]}\\n\" + \\\n",
        "      f\"{type(x_train_padded)}, {x_train_padded.shape}\\n\" + \\\n",
        "      f\"{y_train.shape}\\n\")\n",
        "\n",
        "# Valid set preprocessed --> Dict({word: Id}) --> Matrix\n",
        "valid_sequences = tokenizer.texts_to_sequences(X_valid)\n",
        "x_valid_padded  = pad_sequences(valid_sequences, maxlen=max_length, value=0)\n",
        "\n",
        "print(f\"Valid set:\\n\" + \\\n",
        "      f\"{type(valid_sequences)}, {valid_sequences[0]}\\n\" + \\\n",
        "      f\"{type(x_valid_padded)}, {x_valid_padded.shape}\\n\"+ \\\n",
        "      f\"{y_valid.shape}\\n\")\n",
        "\n",
        "\n",
        "# test set preprocessed --> Dict({word: Id}) --> Matrix\n",
        "test_sequences = tokenizer.texts_to_sequences(df_test_preproc['preprocessed_text'])\n",
        "x_test_padded  = pad_sequences(test_sequences, maxlen=max_length, value=0)\n",
        "\n",
        "print(f\"Test set:\\n\" + \\\n",
        "      f\"{type(test_sequences)}, {test_sequences[0]}\\n\" + \\\n",
        "      f\"{type(x_test_padded)}, {x_test_padded.shape}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOEPbfyLDCIe"
      },
      "source": [
        "# Models (Tensorflow embedding + LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ywvq61S8z_2i"
      },
      "source": [
        "# Create the model\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,recurrent_dropout=0.3, dropout=0.3, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,recurrent_dropout=0.1, dropout=0.1)),\n",
        "    tf.keras.layers.Dense(512, activation = \"relu\"),\n",
        "    tf.keras.layers.Dense(1, activation = \"sigmoid\")\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZZkj4PQ3M2f"
      },
      "source": [
        "# Train the data\n",
        "model.fit(x_train_padded, y_train, batch_size=128, epochs=20, validation_data=(x_valid_padded, y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f51epXK6Dnxm"
      },
      "source": [
        "# Evaluation of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acIn2a-l3_Im"
      },
      "source": [
        "# 1. Evaluate the validation set\n",
        "valid_scores     = model.evaluate(x_valid_padded, y_valid, verbose=0)\n",
        "\n",
        "valid_predictions = model.predict(x_valid_padded, verbose=0)\n",
        "\n",
        "print(\"Validation set:\\nAccuracy: %.2f%%\" % (valid_scores[1] * 100))\n",
        "\n",
        "print(f\"Non-sarcasm: {(np.floor(valid_predictions * 2) == 0).sum()}\")\n",
        "print(f\"Sarcasm:  {(np.floor(valid_predictions * 2) == 1).sum()}\")\n",
        "\n",
        "print(metrics.classification_report(np.floor(valid_predictions * 2), y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FdopGYZ_KfT"
      },
      "source": [
        "# 2. Evaluation of the test set \n",
        "\n",
        "test_predictions = model.predict(x_test_padded, verbose=0)\n",
        "df_test_preproc['predictions'] = np.floor(np.floor(test_predictions * 2)\n",
        "\n",
        "print(f\"Non-sarcasm: {(np.floor(test_predictions * 2) == 0).sum()}\")\n",
        "print(f\"Sarcasm:  {(np.floor(test_predictions * 2) == 1).sum()}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVqpMYELTBic"
      },
      "source": [
        " # Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2z3Wx85CBF_"
      },
      "source": [
        "df_test_preproc[['Id', 'predictions']].to_csv('results_emb_lstm.csv') \n",
        "\n",
        "files.download('results_emb_lstm.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWuKrGULSkzs"
      },
      "source": [
        "# Model 2: distillBERT embedding + SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hvMkhPxEhpL"
      },
      "source": [
        "tokenizer  = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqdTYzmXSwvT"
      },
      "source": [
        "def get_embedding(x):\n",
        "   inputs = tokenizer(x, return_tensors=\"pt\")\n",
        "   outputs = bert_model(**inputs)\n",
        "   last_hidden_states = outputs.last_hidden_state\n",
        "   return last_hidden_states.mean(axis=1).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFPSZNMvSwzG"
      },
      "source": [
        "with torch.no_grad():\n",
        "    embedding_layer = df_train_preproc['preprocessed_text'].progress_apply(lambda x: get_embedding(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boU61ucmeI9v"
      },
      "source": [
        "embedding_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKPX31Ecg-YJ"
      },
      "source": [
        "data_train = np.concatenate(embedding_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYNFDWGUSw1M"
      },
      "source": [
        "X_train1, X_valid1, y_train1, y_valid1 = train_test_split(data_train, \n",
        "                                                    df_train_preproc['is_sarcastic'], \n",
        "                                                    test_size=0.2, shuffle=True, random_state=42)\n",
        "X_train1.shape, X_valid1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoE2dVTGhQzm"
      },
      "source": [
        "clf = LinearSVC()\n",
        "\n",
        "clf.fit(X_train1, y_train1)\n",
        "\n",
        "(clf.predict(X_train1) == y_train1).mean(), (clf.predict(X_valid1) == y_valid1).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8svVdz90iWOZ"
      },
      "source": [
        "with torch.no_grad():\n",
        "    embedding_layer_test = df_test_preproc['preprocessed_text'].progress_apply(lambda x: get_embedding(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U91JoRimhTQi"
      },
      "source": [
        "df_test_preproc['predictions_svm'] = clf.predict(embedding_layer_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCibkPoQ0Yip"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}